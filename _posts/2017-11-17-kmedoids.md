---
layout: post
title: "K-medoids"
---

{{ page.title }}
================

K-medoids is very similar to K-means. The difference is that K-means is updating $$\mu_{k}$$ according to   

$$\mu_{k} = \frac{\sum_n r_{nk}x_{n}}{\sum_n r_{nk}}$$  

, while K-memoids updates $$\mu_{k}$$ using one of the samples in that cluster, in another saying, median.

So in cases where we can't use Euclidian distance to describe the dissmilarity between samples, by using K-medoids we can still choose one of the samples in the cluster. For example, student A is from Stanford, student B is from UCB, student C is from UCLA and they belong to the same cluster California. It is meaningless to calculate a 'mean' value of these three universities. But we maybe we can choose student B (median? Lol)as the new center point of the cluster California.
In K-modoids,

$$ J = \sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk} V(x_{n}, \mu_{k}), \quad r_{nk} = \{0, 1\} $$  

Ususlly $$V$$ is a dissimilarity matrix $$D$$, where $$D_{ij}$$ represents the dissimilarity beween sample $$i$$ and sample $$j$$.  
**Note: Because we are choosing a new $$\mu_{k}$$ from the sample points so the requirement of data is not as high as K-means for K-medoid, in another saying, K-medoid is robuster to training data. But the time complexity will increase from $$O(N)$$ to $$O(N^2)$$.**