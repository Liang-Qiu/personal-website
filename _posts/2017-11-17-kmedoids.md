---
layout: post
title: "K-medoids"
---

{{ page.title }}
================

K-medoids is very similar to K-means. The difference is that K-means updates $$\mu_k$$ according to   

$$\mu_k = \frac{\sum_n r_{nk}x_n}{\sum_n r_{nk}}$$  

, while K-memoids updates $$\mu_k$$ using one of the samples in that cluster, in another saying, median.

In cases where we can't use Euclidian distance to describe the dissmilarity between samples, using K-medoids allows us to choose one of the samples in the cluster. For example, student A is from Stanford, student B is from UCB, student C is from UCLA and they belong to the same cluster California. It is meaningless to calculate a 'mean' value of these three universities. But we can still choose student B (median? Lol)as the center point of the cluster California.
In K-modoids,

$$ J = \sum_{n=1}^N\sum_{k=1}^Kr_{nk} V(x_n, \mu_k), \quad r_{nk} = \{0, 1\} $$  

Ususlly $$V$$ is a dissimilarity matrix $$D$$, where $$D_{ij}$$ represents the dissimilarity beween sample $$i$$ and sample $$j$$.  
**Note: Because we are choosing a new $$\mu_{k}$$ from the sample points so the data requirement of K-medoid is not as high as K-means, in another saying, K-medoid is robuster to training data. But the time complexity will increase from $$O(N)$$ to $$O(N^2)$$.**